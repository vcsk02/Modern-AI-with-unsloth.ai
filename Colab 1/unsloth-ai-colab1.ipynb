{"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python310"},"language_info":{"name":"python","version":"3.10"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"855cdb8d-9e05-43c6-8836-9bf13067b795","cell_type":"markdown","source":"# (a) Full Finetuning ‚Äî SmolLM2‚Äë135M (Transformers)\n**Created:** 2025-11-10 02:42 UTC\n\nThis notebook performs *full-parameter fine‚Äëtuning* on the tiny **SmolLM2‚Äë135M** model using a miniature toy dataset so it runs fast in Kaggle.\nWe also install **Unsloth** (for later notebooks) and show how to apply a chat template, but the actual full fine‚Äëtuning here uses vanilla ü§ó Transformers since it's a very small model.\n\n> **Checklist for your recording**  \n> 1) Show GPU is enabled (Settings ‚Üí Accelerator: GPU, Internet: On).  \n> 2) Walk through the dataset format and preprocessing.  \n> 3) Start training (just a few hundred steps).  \n> 4) Show sample generations before/after.  \n> 5) Save + download the model.","metadata":{}},{"id":"41b0eeaa-01a9-46cd-961c-3e877a647071","cell_type":"code","source":"!pip -q install --upgrade pip\n!pip -q install \"transformers>=4.44.2\" \"datasets>=2.19.0\" \"accelerate>=0.33.0\" \"evaluate\" \"peft\" \"trl\" \"bitsandbytes\" \"unsloth>=2024.11.0\"","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T02:44:31.990277Z","iopub.execute_input":"2025-11-10T02:44:31.990586Z","iopub.status.idle":"2025-11-10T02:48:41.343893Z","shell.execute_reply.started":"2025-11-10T02:44:31.990564Z","shell.execute_reply":"2025-11-10T02:48:41.342920Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"c94daef6-ab76-4b35-9c05-bc3ceeecf69d","cell_type":"code","source":"import torch, platform, os, json, random\nprint(\"Python:\", platform.python_version())\nprint(\"PyTorch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T02:52:02.694957Z","iopub.execute_input":"2025-11-10T02:52:02.695794Z","iopub.status.idle":"2025-11-10T02:52:04.553392Z","shell.execute_reply.started":"2025-11-10T02:52:02.695748Z","shell.execute_reply":"2025-11-10T02:52:04.552747Z"}},"outputs":[{"name":"stdout","text":"Python: 3.11.13\nPyTorch: 2.8.0+cu128\nCUDA available: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":2},{"id":"c4840650-87e7-4b31-b790-6c503e4e2255","cell_type":"markdown","source":"## Build a tiny toy chat dataset\nWe keep it super small so the demo finishes quickly; replace with your real dataset for a longer run. The format becomes a single string field `text` after applying a simple template.","metadata":{}},{"id":"afeea8cc-ce7d-44d0-9947-75ab6885c434","cell_type":"code","source":"from datasets import Dataset\n\npairs = [\n    {\"instruction\":\"Write a Python function to add two numbers a and b.\",\"response\":\"def add(a,b):\\n    return a+b\"},\n    {\"instruction\":\"Explain binary search in 2 sentences.\",\"response\":\"Binary search repeatedly halves a sorted range to find a target. It runs in O(log n) time.\"},\n    {\"instruction\":\"Generate a short pep talk for learning algorithms.\",\"response\":\"Keep tinkering. Mistakes are breadcrumbs toward understanding‚Äîfollow them.\"},\n    {\"instruction\":\"Fix the bug: def f(x): return x*2 if x>10: return 0\",\"response\":\"def f(x):\\n    if x>10:\\n        return 0\\n    return x*2\"},\n]\n\ndef simple_template(example):\n    prompt = f\"<|system|>You are a helpful coding assistant.</s>\\n<|user|>{example['instruction']}</s>\\n<|assistant|>{example['response']}\"\n    return {\"text\": prompt}\n\nraw_ds = Dataset.from_list(pairs)\nds = raw_ds.map(simple_template, remove_columns=raw_ds.column_names)\nds = ds.train_test_split(test_size=0.25, seed=42)\nds","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T02:52:08.363345Z","iopub.execute_input":"2025-11-10T02:52:08.363925Z","iopub.status.idle":"2025-11-10T02:52:10.208829Z","shell.execute_reply.started":"2025-11-10T02:52:08.363902Z","shell.execute_reply":"2025-11-10T02:52:10.208229Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9811c012372448b3b0af2f1ac1d331f4"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 3\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 1\n    })\n})"},"metadata":{}}],"execution_count":3},{"id":"96351bcb-4045-4eef-b1d6-b7115c887b43","cell_type":"markdown","source":"## Load **SmolLM2‚Äë135M** and tokenize","metadata":{}},{"id":"fc712201-8913-406e-b2c5-b9fc34f46724","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name = \"HuggingFaceTB/SmolLM2-135M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef tok(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=512)\ntokenized = ds.map(tok, batched=True, remove_columns=[\"text\"])\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.use_cache = False\nmodel","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T02:52:13.807654Z","iopub.execute_input":"2025-11-10T02:52:13.807967Z","iopub.status.idle":"2025-11-10T02:52:41.342810Z","shell.execute_reply.started":"2025-11-10T02:52:13.807945Z","shell.execute_reply":"2025-11-10T02:52:41.342138Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af8f39a5ee19462e932549b970122b33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e320c50a5bd4248ad4ab79f27883f62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60caad4b1406461c902113f9f29db43b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac4b77934b94076af04eab7ed93bf28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6895c86c98de4db98e670fcc597ae5db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a93d028f4f74282b4f1ebb04631e810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b2cf758dc6e460da648b776ed2b32fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b628d04d4004e78a819fa87213430b5"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2025-11-10 02:52:20.945286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762743141.133701      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762743141.187393      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8cef64c6f34d7cb43baf40d7bbe70b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37519b2677444fcd8badf3a3d4854f83"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 576)\n    (layers): ModuleList(\n      (0-29): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((576,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":4},{"id":"75fc55e0-b990-4665-9223-c075fce50a72","cell_type":"markdown","source":"## Train (full‚Äëparameter fine‚Äëtuning)\nWe keep steps tiny so it completes quickly on Kaggle. Increase `num_train_epochs` or `max_steps` for a real run.","metadata":{}},{"id":"546f11e3-e86f-49d6-82c7-86b2e85c7665","cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\nimport transformers, torch\nprint(\"Transformers:\", transformers.__version__)  # just to show in your recording\n\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/smollm2_fullft\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=1,\n    bf16=torch.cuda.is_available(),\n    learning_rate=5e-4,\n    warmup_steps=10,\n    logging_steps=5,\n    # NEW-style flags (>=4.47):\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",          # so save_steps takes effect\n    logging_strategy=\"steps\",       # optional but tidy\n    eval_steps=20,\n    save_steps=50,\n    max_steps=120,\n    report_to=\"none\",\n)\n\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"],\n    data_collator=collator,\n)\ntrain_result = trainer.train()\ntrain_result\n","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T02:54:04.004680Z","iopub.execute_input":"2025-11-10T02:54:04.004997Z","iopub.status.idle":"2025-11-10T02:54:41.630241Z","shell.execute_reply.started":"2025-11-10T02:54:04.004977Z","shell.execute_reply":"2025-11-10T02:54:41.629543Z"}},"outputs":[{"name":"stdout","text":"Transformers: 4.57.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 00:35, Epoch 120/120]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.035900</td>\n      <td>2.921495</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.019500</td>\n      <td>3.050814</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.019200</td>\n      <td>3.084465</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.019200</td>\n      <td>3.092580</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.019200</td>\n      <td>3.109593</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.019200</td>\n      <td>3.093855</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=120, training_loss=0.17629979513585567, metrics={'train_runtime': 36.8776, 'train_samples_per_second': 52.064, 'train_steps_per_second': 3.254, 'total_flos': 13993367362560.0, 'train_loss': 0.17629979513585567, 'epoch': 120.0})"},"metadata":{}}],"execution_count":6},{"id":"e77a69aa-9ccb-4e7f-82cc-8d894507b006","cell_type":"markdown","source":"## Quick smoke test","metadata":{}},{"id":"ea894381-a2e0-4093-bbdb-7f1be4f6a8d1","cell_type":"code","source":"from transformers import TextStreamer\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\nprompt = \"<|system|>You are a helpful coding assistant.</s>\\n<|user|>Write a Python function to compute factorial.</s>\\n<|assistant|>\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    _ = model.generate(**inputs, max_new_tokens=128, do_sample=True, temperature=0.7, streamer=streamer)","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T02:55:10.747044Z","iopub.execute_input":"2025-11-10T02:55:10.747642Z","iopub.status.idle":"2025-11-10T02:55:15.909194Z","shell.execute_reply.started":"2025-11-10T02:55:10.747617Z","shell.execute_reply":"2025-11-10T02:55:15.908636Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"def factorial(n):\n    return n * factorial(n-1)\n<|assistant|>Write a Python function to add two numbers a and b.</s>\n<|assistant|>def add(a,b):\n    return a+b\n<|assistant|>def add(a,b):\n    return a+b\n<|assistant|>def add(a,b):\n    return a+b\n<|assistant|>def add(a,b):\n    return a+b\n<|assistant|>def add(a,b\n","output_type":"stream"}],"execution_count":7},{"id":"8e5e8a62-67ad-450e-876b-5dea2aa6fe5f","cell_type":"code","source":"trainer.save_model(\"/kaggle/working/smollm2_fullft\")\ntokenizer.save_pretrained(\"/kaggle/working/smollm2_fullft\")\nprint(\"Saved to /kaggle/working/smollm2_fullft\")","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T02:55:20.751066Z","iopub.execute_input":"2025-11-10T02:55:20.751762Z","iopub.status.idle":"2025-11-10T02:55:21.853824Z","shell.execute_reply.started":"2025-11-10T02:55:20.751736Z","shell.execute_reply":"2025-11-10T02:55:21.852927Z"}},"outputs":[{"name":"stdout","text":"Saved to /kaggle/working/smollm2_fullft\n","output_type":"stream"}],"execution_count":8}]}