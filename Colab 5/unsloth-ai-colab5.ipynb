{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7f1dd661-0391-4c78-a85a-b8103398fe35","cell_type":"markdown","source":"# (e) Continued Pretraining — Teach a Tiny New Language\n**Created:** 2025-11-10 02:42 UTC\n\nWe demonstrate **Continued Pretraining** by feeding synthetic text in a toy language called *Quirkish*. The model learns new tokens/patterns.\nFor larger corpora and tokenizers, see Unsloth's Continued Pretraining docs.","metadata":{}},{"id":"8963b304-0951-4bf1-9ea7-5cc9c5cc20b7","cell_type":"code","source":"!pip -q install --upgrade pip\n!pip -q install \"transformers>=4.44.2\" \"datasets>=2.19.0\" \"accelerate>=0.33.0\" \"unsloth>=2024.11.0\"","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:19:37.622372Z","iopub.execute_input":"2025-11-10T04:19:37.622621Z","iopub.status.idle":"2025-11-10T04:23:33.850324Z","shell.execute_reply.started":"2025-11-10T04:19:37.622597Z","shell.execute_reply":"2025-11-10T04:23:33.849529Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"5fa376a5-8d28-4a77-9004-b2a4cb57d75e","cell_type":"code","source":"import torch, random\nprint(\"CUDA:\", torch.cuda.is_available())","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:23:46.938443Z","iopub.execute_input":"2025-11-10T04:23:46.939171Z","iopub.status.idle":"2025-11-10T04:23:48.754289Z","shell.execute_reply.started":"2025-11-10T04:23:46.939134Z","shell.execute_reply":"2025-11-10T04:23:48.753684Z"}},"outputs":[{"name":"stdout","text":"CUDA: True\n","output_type":"stream"}],"execution_count":2},{"id":"f44f823e-e27e-4b13-827d-13d901991ab4","cell_type":"markdown","source":"## Build toy raw text","metadata":{}},{"id":"3544c9a8-b46c-47d5-982c-fbe42df77628","cell_type":"code","source":"from datasets import Dataset\n\nquirkish = [\n    \"zor blip mako. quori zen taf. noro blip blip mako!\",\n    \"mako-lin quori lin-lin. zor quori zen?\",\n    \"zen zor mako mako. blip norin quori zor.\",\n    \"blip-zen mako norin zor. quori zen blip.\",\n    \"noro blip mako zor zen. quori norin mako!\",\n]\nds = Dataset.from_list([{\"text\": t} for t in quirkish])\nds","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:23:52.059996Z","iopub.execute_input":"2025-11-10T04:23:52.060690Z","iopub.status.idle":"2025-11-10T04:23:53.906772Z","shell.execute_reply.started":"2025-11-10T04:23:52.060665Z","shell.execute_reply":"2025-11-10T04:23:53.906136Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 5\n})"},"metadata":{}}],"execution_count":3},{"id":"f9ad652a-181c-4548-8cc1-451fa02a2c0a","cell_type":"markdown","source":"## Tokenize and run continued pretraining","metadata":{}},{"id":"f4d33c47-fd1d-4835-a592-e429f6f9abb8","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nbase = \"HuggingFaceTB/SmolLM2-135M\"\ntokenizer = AutoTokenizer.from_pretrained(base)\nif tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n\ndef tok(b): return tokenizer(b[\"text\"], truncation=True, max_length=256)\ntok_ds = ds.map(tok, batched=True, remove_columns=[\"text\"])\n\nmodel = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\nmodel.resize_token_embeddings(len(tokenizer))\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/smollm2_cpt\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    learning_rate=5e-5,\n    max_steps=200,\n    bf16=torch.cuda.is_available(),\n    logging_steps=10,\n    report_to=\"none\",\n)\ntrainer = Trainer(model=model, args=args, train_dataset=tok_ds, data_collator=collator)\n_ = trainer.train()\ntrainer.save_model(\"/kaggle/working/smollm2_cpt\")\ntokenizer.save_pretrained(\"/kaggle/working/smollm2_cpt\")","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:23:57.433993Z","iopub.execute_input":"2025-11-10T04:23:57.434779Z","iopub.status.idle":"2025-11-10T04:25:41.415123Z","shell.execute_reply.started":"2025-11-10T04:23:57.434753Z","shell.execute_reply":"2025-11-10T04:25:41.414243Z"}},"outputs":[{"name":"stderr","text":"2025-11-10 04:24:03.245454: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762748643.452738      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762748643.509056      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc64d9b29f9042839c2fd4f1a36ea898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec31f79fe03b4fdc83a01f1a35773dd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6ace23588524fb4b3c86b164640a5a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f7ddcdcafe4b759d2de794421f99de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adff362684244306815fd7240be0d55d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5be44170b51d41548601b111d38ceeb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6129ce8c9f1416fad8ccd7b22c88bc0"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"386c013bde384e13b3b6c5a91903a662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33d4cca461b451f97988b4aca991deb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 01:10, Epoch 200/200]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>5.340500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>4.265600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.460900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.655900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.884600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.133700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.547700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.257500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.106000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.052600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.037000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.027500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.024500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.021900</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.018400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.017900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/smollm2_cpt/tokenizer_config.json',\n '/kaggle/working/smollm2_cpt/special_tokens_map.json',\n '/kaggle/working/smollm2_cpt/vocab.json',\n '/kaggle/working/smollm2_cpt/merges.txt',\n '/kaggle/working/smollm2_cpt/added_tokens.json',\n '/kaggle/working/smollm2_cpt/tokenizer.json')"},"metadata":{}}],"execution_count":4},{"id":"6a66f368-9efe-4c35-b187-25105da5f81a","cell_type":"markdown","source":"## Sample generation in the *Quirkish* style","metadata":{}},{"id":"4136cf0e-68fb-47ad-ad53-7103e4b37e97","cell_type":"code","source":"from transformers import TextStreamer\nstreamer = TextStreamer(tokenizer, skip_special_tokens=True)\nprompt = \"zor blip\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    _ = model.generate(**inputs, max_new_tokens=60, do_sample=True, temperature=0.9, streamer=streamer)","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:25:58.199759Z","iopub.execute_input":"2025-11-10T04:25:58.200671Z","iopub.status.idle":"2025-11-10T04:26:00.989993Z","shell.execute_reply.started":"2025-11-10T04:25:58.200642Z","shell.execute_reply":"2025-11-10T04:26:00.989416Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"zor blip mako. quori zen taf. noro blip blip mako!\n\nRafi Letzner: (1:04) You like it, so much, so much. (laughter) I'll be fine with that. If we have to talk, we\n","output_type":"stream"}],"execution_count":5}]}