{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b78bde7e-77b4-4ca1-98d5-686305c73751","cell_type":"markdown","source":"# (d) Reinforcement Learning — GRPO (Reasoning)\n**Created:** 2025-11-10 02:42 UTC\n\nThis notebook follows the **GRPO** (Group Relative Policy Optimization) recipe to nudge a model toward reasoning‑style outputs.\nWe use a tiny arithmetic dataset and **SmolLM2‑135M** for speed. For larger models (e.g., Llama‑3.1 8B), see Unsloth's GRPO tutorial.","metadata":{}},{"id":"983996ff-6ecf-4a97-afb2-4ad7d73937ea","cell_type":"code","source":"!pip -q install --upgrade pip\n!pip -q install \"transformers>=4.44.2\" \"datasets>=2.19.0\" \"accelerate>=0.33.0\" \"trl>=0.9.6\" \"unsloth>=2024.11.0\"","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:07:56.505438Z","iopub.execute_input":"2025-11-10T04:07:56.505692Z","iopub.status.idle":"2025-11-10T04:11:48.403826Z","shell.execute_reply.started":"2025-11-10T04:07:56.505664Z","shell.execute_reply":"2025-11-10T04:11:48.402898Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"74614a4c-5d54-4b9b-a9d0-84ce0aeb0429","cell_type":"code","source":"import torch, random\nrandom.seed(0)\nprint(\"CUDA:\", torch.cuda.is_available())","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:11:52.758022Z","iopub.execute_input":"2025-11-10T04:11:52.758333Z","iopub.status.idle":"2025-11-10T04:11:54.779414Z","shell.execute_reply.started":"2025-11-10T04:11:52.758297Z","shell.execute_reply":"2025-11-10T04:11:54.778532Z"}},"outputs":[{"name":"stdout","text":"CUDA: True\n","output_type":"stream"}],"execution_count":2},{"id":"896a6763-be11-4e8f-92bc-1fd277c31553","cell_type":"markdown","source":"## Tiny math reasoning dataset\nWe ask the model to think step‑by‑step using a deliberate reasoning format.","metadata":{}},{"id":"0a41ab80-9fe5-4ff2-8174-bed6a8b27f66","cell_type":"code","source":"from datasets import Dataset\nsamples = []\nfor a in range(11, 16):\n    for b in range(3, 6):\n        ans = a*b\n        prompt = f\"Solve: {a} * {b}. Respond with your reasoning and final answer as 'Answer: <num>'.\"\n        reason = f\"Multiply {a} by {b}. {a}*{b}={ans}. Answer: {ans}\"\n        samples.append({\"prompt\": prompt, \"answer\": reason, \"answer_only\": f\"Answer: {ans}\"})\nds = Dataset.from_list(samples)\nds","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:11:57.933220Z","iopub.execute_input":"2025-11-10T04:11:57.933616Z","iopub.status.idle":"2025-11-10T04:12:00.130386Z","shell.execute_reply.started":"2025-11-10T04:11:57.933589Z","shell.execute_reply":"2025-11-10T04:12:00.129803Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'answer', 'answer_only'],\n    num_rows: 15\n})"},"metadata":{}}],"execution_count":3},{"id":"7f2007fb-b6ab-4192-ac37-4d696abab2a1","cell_type":"markdown","source":"## Policy + reference + reward\nWe craft a simple reward: +1 if model outputs the correct final answer token, else 0. This is intentionally minimal for speed.","metadata":{}},{"id":"4d0ee3a8-1586-4ea8-937c-0f825970d0ea","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import GRPOTrainer, GRPOConfig\n\nbase = \"HuggingFaceTB/SmolLM2-135M\"\ntokenizer = AutoTokenizer.from_pretrained(base)\nif tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\npolicy = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\nref = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\n\ndef reward_fn(samples, **kwargs):\n    # samples: list of strings (model outputs). Reward 1.0 if contains correct 'Answer: X' substring.\n    rewards = []\n    for s, ex in zip(samples, kwargs.get(\"inputs\", [])):\n        target = ex[\"answer_only\"]\n        rewards.append(1.0 if target in s else 0.0)\n    return rewards\n\ngrpo_args = GRPOConfig(\n    output_dir=\"/kaggle/working/smollm2_grpo\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    learning_rate=5e-6,\n    max_steps=80,\n    bf16=torch.cuda.is_available(),\n    logging_steps=5,\n)\ntrainer = GRPOTrainer(\n    model=policy,\n    ref_model=ref,\n    args=grpo_args,\n    reward_funcs=[reward_fn],\n    tokenizer=tokenizer,\n    max_prompt_length=128,\n    max_completion_length=96,\n    train_dataset=ds,\n)\ntrainer.train()\ntrainer.save_model(\"/kaggle/working/smollm2_grpo\")\ntokenizer.save_pretrained(\"/kaggle/working/smollm2_grpo\")","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:12:02.923224Z","iopub.execute_input":"2025-11-10T04:12:02.923781Z","iopub.status.idle":"2025-11-10T04:12:33.487869Z","shell.execute_reply.started":"2025-11-10T04:12:02.923754Z","shell.execute_reply":"2025-11-10T04:12:33.486839Z"}},"outputs":[{"name":"stderr","text":"2025-11-10 04:12:08.627726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762747928.835687      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762747928.891434      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:283: UserWarning: \n    Found GPU0 Tesla P100-PCIE-16GB which is of cuda capability 6.0.\n    Minimum and Maximum cuda capability supported by this version of PyTorch is\n    (7.0) - (12.0)\n    \n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:304: UserWarning: \n    Please install PyTorch with a following CUDA\n    configurations:  12.6 following instructions at\n    https://pytorch.org/get-started/locally/\n    \n  warnings.warn(matched_cuda_warn.format(matched_arches))\n/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:326: UserWarning: \nTesla P100-PCIE-16GB with CUDA capability sm_60 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\nIf you want to use the Tesla P100-PCIE-16GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4a39385589f4a15a64b911e29bb6e3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4996bae96784cfdba40de0daf76564a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f95c14068fb4303949c556497d72ae4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0925741204d40c5babe7bad2da03b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd387263e474880b211739c14963533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23b8829c9c74d8386b58862393c03c9"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dc82715549a46168e2bace46e2f26b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e0dd17aea24f5aa537fa3fdf5ee787"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2237827451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m grpo_args = GRPOConfig(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/kaggle/working/smollm2_grpo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_config.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, model_init_kwargs, disable_dropout, max_prompt_length, num_generations, max_completion_length, ds3_gather_for_generation, shuffle_dataset, generation_batch_size, steps_per_generation, temperature, top_p, top_k, min_p, generation_kwargs, repetition_penalty, use_transformers_paged, cache_implementation, use_vllm, vllm_mode, vllm_model_impl, vllm_enable_sleep_mode, vllm_guided_decoding_regex, vllm_server_base_url, vllm_server_host, vllm_server_port, vllm_server_timeout, vllm_gpu_memory_utilization, vllm_tensor_parallel_size, beta, num_iterations, epsilon, delta, epsilon_high, importance_sampling_level, reward_weights, scale_rewards, loss_type, mask_truncated_completions, sync_ref_model, ref_model_mixup_alpha, ref_model_sync_steps, top_entropy_quantile, use_liger_loss, vllm_importance_sampling_correction, vllm_importance_sampling_cap, log_completions, num_completions_to_print, wandb_log_unique_prompts)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_config.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# num_generations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_batch_size\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_generations\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    680\u001b[0m                 \u001b[0;34mf\"generation_batch_size ({self.generation_batch_size}) must be divisible by num_generations \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;34mf\"({self.num_generations}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: generation_batch_size (4) must be divisible by num_generations (8)."],"ename":"ValueError","evalue":"generation_batch_size (4) must be divisible by num_generations (8).","output_type":"error"}],"execution_count":4},{"id":"c9769ac0-6a8b-4ea5-b458-a258ceae29ed","cell_type":"code","source":"test = \"Solve: 13 * 4. Respond with your reasoning and final answer as 'Answer: <num>'.\"\ninputs = tokenizer(test, return_tensors=\"pt\").to(policy.device)\nwith torch.no_grad():\n    out = policy.generate(**inputs, max_new_tokens=96, do_sample=True, temperature=0.7)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:12:39.540281Z","iopub.execute_input":"2025-11-10T04:12:39.540598Z","iopub.status.idle":"2025-11-10T04:12:44.874583Z","shell.execute_reply.started":"2025-11-10T04:12:39.540576Z","shell.execute_reply":"2025-11-10T04:12:44.873713Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Solve: 13 * 4. Respond with your reasoning and final answer as 'Answer: <num>'.\n\n15. I am 22 years old. In 2010, I started working as a software engineer at a major company. In 2013, I was promoted to Software Engineer and now I am a software developer at a small company. In the last 5 years, I have been working with a new startup that is trying to develop a new product called \"The Greatest New Product\". I know most of my teammates are from the\n","output_type":"stream"}],"execution_count":5}]}