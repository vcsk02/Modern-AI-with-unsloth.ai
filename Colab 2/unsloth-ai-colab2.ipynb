{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4c63e244-6bce-4b5d-bffd-2be6af9cb7d8","cell_type":"markdown","source":"# (b) LoRA (PEFT) — SmolLM2‑135M\n**Created:** 2025-11-10 02:42 UTC\n\nThis notebook applies **LoRA** (parameter‑efficient fine‑tuning) to the same SmolLM2‑135M model. We keep the dataset tiny to finish fast.","metadata":{}},{"id":"ca69a649-605f-4349-a0ee-92538590528f","cell_type":"code","source":"!pip -q install --upgrade pip\n!pip -q install \"transformers>=4.44.2\" \"datasets>=2.19.0\" \"accelerate>=0.33.0\" \"peft>=0.12.0\" \"trl\" \"bitsandbytes\" \"unsloth>=2024.11.0\"","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T03:14:48.344224Z","iopub.execute_input":"2025-11-10T03:14:48.344913Z","iopub.status.idle":"2025-11-10T03:18:44.580231Z","shell.execute_reply.started":"2025-11-10T03:14:48.344886Z","shell.execute_reply":"2025-11-10T03:18:44.579260Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"510be17d-1202-421c-b5a2-065bcca41661","cell_type":"code","source":"import torch, platform\nprint(\"Python:\", platform.python_version())\nprint(\"Torch:\", torch.__version__)\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T03:18:48.714550Z","iopub.execute_input":"2025-11-10T03:18:48.715043Z","iopub.status.idle":"2025-11-10T03:18:50.970464Z","shell.execute_reply.started":"2025-11-10T03:18:48.715005Z","shell.execute_reply":"2025-11-10T03:18:50.969750Z"}},"outputs":[{"name":"stdout","text":"Python: 3.11.13\nTorch: 2.8.0+cu128\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":2},{"id":"1b0eaef3-0b60-4d57-ada8-2fb803b4f297","cell_type":"code","source":"from datasets import Dataset\npairs = [\n    {\"instruction\":\"Summarize: `merge sort` algorithm.\",\"response\":\"Split array, sort halves recursively, and merge. O(n log n).\"},\n    {\"instruction\":\"Write a unit test for add(a,b).\",\"response\":\"def test_add():\\n    assert add(2,3)==5\"},\n    {\"instruction\":\"Give 3 bullet points about hashing.\",\"response\":\"- Maps keys to indices\\n- Collisions need handling\\n- O(1) average lookup\"},\n    {\"instruction\":\"Why use gradient accumulation?\",\"response\":\"It simulates larger batch sizes when memory is limited.\"},\n]\ndef simple_template(example):\n    return {\"text\": f\"### Instruction\\n{example['instruction']}\\n\\n### Response\\n{example['response']}\"}\nraw_ds = Dataset.from_list(pairs)\nds = raw_ds.map(simple_template, remove_columns=raw_ds.column_names).train_test_split(test_size=0.25, seed=7)\nds","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T03:18:56.127589Z","iopub.execute_input":"2025-11-10T03:18:56.128262Z","iopub.status.idle":"2025-11-10T03:18:57.997179Z","shell.execute_reply.started":"2025-11-10T03:18:56.128235Z","shell.execute_reply":"2025-11-10T03:18:57.996628Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7e46b17f6994fab9af39613ce722c2d"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 3\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 1\n    })\n})"},"metadata":{}}],"execution_count":3},{"id":"db5c8070-45b9-4c59-92d2-4919f433e127","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name = \"HuggingFaceTB/SmolLM2-135M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ndef tok(b): return tokenizer(b[\"text\"], truncation=True, max_length=512)\ntokenized = ds.map(tok, batched=True, remove_columns=[\"text\"])\nbase_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\nbase_model.resize_token_embeddings(len(tokenizer))\nbase_model.config.use_cache = False","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T03:19:02.565929Z","iopub.execute_input":"2025-11-10T03:19:02.566270Z","iopub.status.idle":"2025-11-10T03:19:29.582448Z","shell.execute_reply.started":"2025-11-10T03:19:02.566237Z","shell.execute_reply":"2025-11-10T03:19:29.581667Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f6e1c09058a488ea55e8320faa3bcc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed8cb41174474cafa5a021c3d4aa6c1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa5c138a63f49378d327cc3555d21b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5934684f8a3c45c493b854448f463351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ab7e1939e54415b82b2fb882f8373de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dc8bdeb4e7a4e0c9110deae4998a20d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0d627d17904ec3b186f280d568db1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57922d5a616d4bc5923dd262cb156fac"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2025-11-10 03:19:09.488065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762744749.686655      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762744749.741282      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92748afffdb44cb0ac8fd3590ff7ddf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b89c5d5e03584669845332efdf81ad1b"}},"metadata":{}}],"execution_count":4},{"id":"a7078aa8-8fa1-4955-a57e-3d1bc995e65c","cell_type":"markdown","source":"## Attach LoRA adapters\nWe target all attention projections for a simple, robust default.","metadata":{}},{"id":"89296542-93dc-4731-812a-803e8ab22b2c","cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n    bias=\"none\",\n)\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T03:19:37.180693Z","iopub.execute_input":"2025-11-10T03:19:37.181834Z","iopub.status.idle":"2025-11-10T03:19:40.365819Z","shell.execute_reply.started":"2025-11-10T03:19:37.181795Z","shell.execute_reply":"2025-11-10T03:19:40.364954Z"}},"outputs":[{"name":"stdout","text":"trainable params: 4,884,480 || all params: 139,399,488 || trainable%: 3.5039\n","output_type":"stream"}],"execution_count":5},{"id":"41d8b9ee-a91c-4a68-a417-1aec2d8b0a46","cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\nimport transformers, torch\nprint(\"Transformers:\", transformers.__version__)  # for the recording\n\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working/smollm2_lora\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=1,\n    bf16=torch.cuda.is_available(),\n    learning_rate=2e-4,\n    logging_steps=5,\n    # new-style flags (>=4.47)\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_steps=20,\n    save_steps=50,\n    max_steps=120,\n    report_to=\"none\",\n)\n\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ntrainer = Trainer(\n    model=model,  # your PEFT LoRA-wrapped model\n    args=args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"],\n    data_collator=collator,\n)\ntrain_result = trainer.train()\ntrain_result\n","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T03:20:48.017851Z","iopub.execute_input":"2025-11-10T03:20:48.018583Z","iopub.status.idle":"2025-11-10T03:21:53.506559Z","shell.execute_reply.started":"2025-11-10T03:20:48.018544Z","shell.execute_reply":"2025-11-10T03:21:53.505918Z"}},"outputs":[{"name":"stdout","text":"Transformers: 4.57.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 01:02, Epoch 120/120]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.081500</td>\n      <td>6.216710</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.056500</td>\n      <td>8.933578</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.040400</td>\n      <td>9.076246</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.038500</td>\n      <td>8.980349</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.041100</td>\n      <td>9.026912</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.036700</td>\n      <td>9.065476</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=120, training_loss=0.40854275872310003, metrics={'train_runtime': 64.602, 'train_samples_per_second': 29.72, 'train_steps_per_second': 1.858, 'total_flos': 8638197903360.0, 'train_loss': 0.40854275872310003, 'epoch': 120.0})"},"metadata":{}}],"execution_count":7},{"id":"4f4b9073-4d5c-406d-8afa-eb6ba7076e52","cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/smollm2_lora\")\ntokenizer.save_pretrained(\"/kaggle/working/smollm2_lora\")\nprint(\"Saved LoRA to /kaggle/working/smollm2_lora\")","metadata":{"executionInfo":{},"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T03:21:57.723006Z","iopub.execute_input":"2025-11-10T03:21:57.723318Z","iopub.status.idle":"2025-11-10T03:21:57.986416Z","shell.execute_reply.started":"2025-11-10T03:21:57.723292Z","shell.execute_reply":"2025-11-10T03:21:57.985543Z"}},"outputs":[{"name":"stdout","text":"Saved LoRA to /kaggle/working/smollm2_lora\n","output_type":"stream"}],"execution_count":8},{"id":"80addddf-2960-4768-abc8-ba37a811948a","cell_type":"markdown","source":"### Merge LoRA (optional) and test generation","metadata":{}},{"id":"462b4adc-5bcd-429a-902d-145c0e6b7eb2","cell_type":"code","source":"# Optional: merge weights for export/inference without PEFT\ntry:\n    merged = model.merge_and_unload()\n    merged.save_pretrained(\"/kaggle/working/smollm2_lora_merged\")\n    tokenizer.save_pretrained(\"/kaggle/working/smollm2_lora_merged\")\n    print(\"Merged model saved.\")\nexcept Exception as e:\n    print(\"Merge skipped:\", e)\n\ninputs = tokenizer(\"Instruction: Write a haiku about coding.\\nResponse:\", return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    out = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.8)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"executionInfo":{}},"outputs":[],"execution_count":null}]}